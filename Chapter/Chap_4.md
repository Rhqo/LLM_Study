# Chap. 4 맞춤형 파인튜닝으로 LLM 최적화하기

## 파인튜닝과 전이학습

파인튜닝 과정

딥러닝 모델의 파라미터를 조정해서 특정 작업이나 데이터셋에서 성능을 향상시키는 과정

- 레이블이 지정된 데이터 수집
    - 중복 제거
    - 데이터 분할
    - 훈련 데이터 섞기 (Shuffling)
    - JSONL 형식 생성
- 하이퍼파라미터 선택
    - 학습률
    - 배치 사이즈
    - 에포크 수
- 모델 적응
- 평가와 반복
    - 정확도
    - F1 score
    - MAE (Mean Absolute Error, 평균 절대 오차)
- 모델 구현 및 추가 학습

파운데이션 모델로 사전 훈련된 클로즈드 모델 사용하기

FM (Foundation Model): 사전 학습이 완료된 모델

## OpenAI 파인튜닝 API 살펴보기

GPT-3 파인튜닝 API

Amazon 리뷰 감정 분류

데이터에 대한 지침 및 모범사례

## OpenAI CLI로 맞춤형 예제 준비하기

## OpenAI CLI 설정하기

하이퍼파라미터 선택과 최적화

## 첫번째 파인튜닝 LLM

정량적 지표로 파인튜닝 모델 평가하기

증분 학습: FM 모델의 훈련을 더 작은 단계로 진행해서 새로운 레이블이 지정된 데이터 포인트로 더 많은 훈련 단계/에포크로 업데이트 하는 과정. 더 작은 데이터셋으로 작업하거나 모델의 일반 지식의 일부를 보존하고자 할 때 이상적.

정성적 평가 기술

파인튜닝된 GPT-3 모델을 app에 통합하기

Amazon 리뷰 카테고리 분류

---

## 정리

이번 장에서는 GPT-3를 파인튜닝하는 방법에 대해 알아보았다.

파인튜닝을 하기 전에 API의 요구사항에 따라 데이터를 정리하고 형식을 맞추어 데이터를 준비해야 한다.

`중복 제거`, `데이터 분할`, `훈련 데이터 섞기` , `OpenAI JSONL 형식 생성` 이 그 과정이다.

다음은 `학습률`, `배치 크기`, `훈련 에포크`  등의 하이퍼파라미터를 선택하여,

OpenAI의 API에 따라 데이터와 하이퍼파라미터를 입력하여 모델을 파인튜닝할 수 있다.