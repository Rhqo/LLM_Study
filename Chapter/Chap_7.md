# Chap. 7 파운데이션 모델을 넘어서

## VQA (Visual Question Answer)

ViT, GPT-2, DistillBERT

- 텍스트 프로세서: DistillBERT
    
    BERT로부터 지식 증류를 통해 태어난 더 작고 효율적인 모델
    
- 이미지 프로세서: ViT
    
    이미지 이해를 위해 특별히 설계된 트랜스포머 기반 아키텍처
    
    이미지 전처리 단계에서, 사전 학습과 동일한 데이터셋인 Imagenet의 데이터셋을 사용한다.
    
    사전 훈련과의 일관성, 사전 지식 활용, 개선된 일반화의 이점이 있고,
    
    제한된 유연성, 새 데이터와의 불일치, 사전 훈련 데이터에 대한 과적합의 단점이 있다.
    
- 텍스트 인코더: GPT-2

                      표지판 사진 → 이미지 프로세서  \

                                                                           —> 텍스트 인코더: “STOP”

“이게 무슨 표지판인가요?” → 텍스트 프로세서 /

은닉 상태 투영과 융합

이미지와 텍스트는 각각의 프로세서를 거쳐, 입력의 유용한 특징 표현을 포함하는 출력 텐서를 생성한다.

각각의 텐서의 차원을 해결하기 위해 선형 투영(Projection) 계층을 사용하여 공유 차원 공간에 투영한다.

이렇게 결합하여 생성된 텐서를 디코더(GPT-2)에 입력하여 일관되고 관련 있는 텍스트 답변을 생성한다.

크로스-어텐션 (Cross-attention)

멀티모달 시스템이 텍스트와 이미지 입력 사이의 상호작용 및 생성하고자 하는 출력 텍스트를 학습할 수 있게 해 주는 매커니즘.

다른 모달리티를 처리할 때 한 모달리티의 출력에 얼마나 중요도를 둘 것인지 결정하는 어텐션 점수 계산

입력에서 출력으로 정보를 효과적으로 “통합”할 수 있게 함.

입력 시퀀스가 key, value (이미지와 텍스트 인코더 query의 조합)

출력 시퀀스는 query 입력으로 사용된다.

Query: 어텐션 가중치를 계산하기 위한 토큰

Key & Value: 시퀀스 내의 다른 토큰

맞춤형 멀티모달 모델

- init
    
    세가지 트랜스포머 모델을 인스턴스화
    
    훈련 속도를 높이기 위해 계층을 고정시킬 수 있음
    
- forward
    
    입력을 받아 출력과 손실 값 생성함
    
    - input_ids
        
        텍스트 토큰(입력 토큰)의 입력 id
        
        크기는 [batch_size, sequence_length]
        
    - attention_mask
        
        어떤 입력 토큰을 주목해야 하는지 (1) 무시해야 하는지 (0) 나타냄
        
        주로 입력 시퀀스에서 패딩 토큰을 다루는 데 사용
        
    - decoder_input_ids
        
        디코더 토큰의 입력 id
        
        훈련 중 디코더에 대한 프롬프트로 사용되는 목표 텍스트를 기반으로 생성됨.
        
        생성은 토크나이저에 의해 이루어짐.
        
        크기는 [batch_size, target_sequence_length]
        
    - image_features
        
        배치의 각 샘플에 대한 전처리된 이미지 특징을 포함하는 텐서
        
        크기는 [batch_size, num_features, feature_dimension]
        
    - labels
        
        목표 텍스트에 대한 실제 레이블을 포함하는 텐서
        
        크기는 [batch_size, target_sequence_length]
        

Data: Visual QA

VQA 훈련 과정

결과

## 피드백 기반 강화 학습 (RLF)

FLAN-T5

보상 모델: 감정과 문법 정확도

LLM의 출력을 입력으로 받아 하나의 스칼라로 보상을 피드백

트랜스포머 강화 학습 (TRL)

트랜스포머 모델을 강화 학습으로 훈련하는 데 사용할 수 있는 오픈 소스 라이브러리

PPO (근위 정책 최적화) 사용하여 최적화

RLF 훈련 과정

- 두 가지 모델 인스턴스화
    - 참조 모델: 업데이트 되지 않음
    - 현재 모델: 업데이트 됨
- 데이터 소스로부터 데이터 가져온다
- 두 보상 모델로부터 보상 계산, 두 보상의 가중합으로 단일 스칼라로 집계
- 보상을 TRL 패키지에 전달하여 다음 2가지 계산
    - 보상 시스템에 기초하여 모델을 약간 업데이트 하는 방법
    - 생성된 텍스트가 참조 모델에서 생성된 텍스트와 얼마나 차이가 나는지, 즉 두 결과 간의 KL-Divergence를 계산.
        
        → 두 텍스트간의 차이를 측정하여 기존 모델의 생성 능력에서 너무 멀어지지 않도록 하는 것
        
- 현재 모델을 데이터 배치로부터 업데이트, 보고 시스템(Weights & Biases, wandb)에 로그 기록
- 다시 1단계로

결과

---

## 정리

기존 모델들을 결합하여 새로운 LLM 아키텍처를 구축하였다.

FLAN-T5, ChatGPT, GPT-4, Cohere의 Command 시리즈,

GPT-2, BERT의 기본 모델들을 지도학습 레이블 데이터로 파인튜닝하는 과정에서,

데이터, 모델 아키텍처 등의 창의적인 접근이 필요하며, 이러한 접근의 기초적인 부분을 배웠다.

`VQA`에서는 `순수 언어 모델링`과 `이미지 처리 분야`에서 접근하는 방법을,

`RLF`에서는 `강화 학습`을 사용하여 언어 모델을 파인튜닝하는 방법을 배웠다.