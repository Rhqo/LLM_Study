{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPFfBPf6zqWul0fDetN0g+g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rhqo/LLM_Study/blob/main/fine_tuning_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Amazon review datasets"
      ],
      "metadata": {
        "id": "wQ0khHYDJwng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q\n",
        "# !pip install pyarrow -q\n",
        "!pip install pandas -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU7oAaKLNaGd",
        "outputId": "6b472c34-cd0d-4ee9-e345-3bdd9e7eb198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "hsK5rE6ePeK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "training_df = pd.DataFrame(dataset['train'])\n",
        "validation_df = pd.DataFrame(dataset['test'])"
      ],
      "metadata": {
        "id": "WheJEBg2Q6I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)\n",
        "print(training_df)\n",
        "print(validation_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D8ydf6qRn1o",
        "outputId": "e74787c9-7962-4fd3-9ac6-c9d61ab5a5e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n",
            "                                                    text  label\n",
            "0      I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
            "1      \"I Am Curious: Yellow\" is a risible and preten...      0\n",
            "2      If only to avoid making this type of film in t...      0\n",
            "3      This film was probably inspired by Godard's Ma...      0\n",
            "4      Oh, brother...after hearing about this ridicul...      0\n",
            "...                                                  ...    ...\n",
            "24995  A hit at the time but now better categorised a...      1\n",
            "24996  I love this movie like no other. Another time ...      1\n",
            "24997  This film and it's sequel Barry Mckenzie holds...      1\n",
            "24998  'The Adventures Of Barry McKenzie' started lif...      1\n",
            "24999  The story centers around Barry McKenzie who mu...      1\n",
            "\n",
            "[25000 rows x 2 columns]\n",
            "                                                    text  label\n",
            "0      I love sci-fi and am willing to put up with a ...      0\n",
            "1      Worth the entertainment value of a rental, esp...      0\n",
            "2      its a totally average film with a few semi-alr...      0\n",
            "3      STAR RATING: ***** Saturday Night **** Friday ...      0\n",
            "4      First off let me say, If you haven't enjoyed a...      0\n",
            "...                                                  ...    ...\n",
            "24995  Just got around to seeing Monster Man yesterda...      1\n",
            "24996  I got this as part of a competition prize. I w...      1\n",
            "24997  I got Monster Man in a box set of three films ...      1\n",
            "24998  Five minutes in, i started to feel how naff th...      1\n",
            "24999  I caught this movie on the Sci-Fi channel rece...      1\n",
            "\n",
            "[25000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_df_for_openai(df):\n",
        "  df['prompt'] = df['text']\n",
        "  df['completion'] = df['label']\n",
        "  df.drop_duplicates(subset=['prompt'], inplace=True)\n",
        "\n",
        "  return df[['prompt', 'completion']].sample(len(df))"
      ],
      "metadata": {
        "id": "0X4eAdYeRRbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_df = prepare_df_for_openai(training_df)\n",
        "validation_df = prepare_df_for_openai(validation_df)"
      ],
      "metadata": {
        "id": "RFMEG9lRUKAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_df.to_json('./train.jsonl', orient='records', lines=True)\n",
        "training_df.to_json('./test.jsonl', orient='records', lines=True)"
      ],
      "metadata": {
        "id": "e9tCg_pZUTMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting OpenAI CLI"
      ],
      "metadata": {
        "id": "5R0FJxPFN26v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -q\n",
        "!pip install jq -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOfgulq8N57T",
        "outputId": "65ef03d9-4d09-4067-bc5c-6763e5f108a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/657.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/657.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.6/657.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai tools fine_tunes.prepare_data -f ./train.jsonl\n",
        "!openai tools fine_tunes.prepare_data -f ./test.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNVpPs65N_DE",
        "outputId": "5e8cadac-b99c-494f-c681-9736cfa56956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing...\n",
            "\n",
            "- Your file contains 24904 prompt-completion pairs\n",
            "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
            "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
            "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
            "- There are 3 examples that are very long. These are rows: [19298, 19327, 19521]\n",
            "For conditional generation, and for classification the examples shouldn't be longer than 2048 tokens.\n",
            "- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
            "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
            "\n",
            "Based on the analysis we will perform the following actions:\n",
            "- [Recommended] Remove 3 long examples [Y/n]: y\n",
            "- [Recommended] Add a suffix separator ` ->` to all prompts [Y/n]: y\n",
            "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: y\n",
            "- [Recommended] Would you like to split into training and validation set? [Y/n]: y\n",
            "\n",
            "\n",
            "Your data will be written to a new JSONL file. Proceed [Y/n]: y\n",
            "\n",
            "Wrote modified files to `./train_prepared_train (4).jsonl` and `./train_prepared_valid (4).jsonl`\n",
            "Feel free to take a look!\n",
            "\n",
            "Now use that file when fine-tuning:\n",
            "> openai api fine_tunes.create -t \"./train_prepared_train (4).jsonl\" -v \"./train_prepared_valid (4).jsonl\" --compute_classification_metrics --classification_positive_class \" 1\"\n",
            "\n",
            "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` ->` for the model to start generating completions, rather than continuing with the prompt.\n",
            "Once your model starts training, it'll approximately take 10.0 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n",
            "Analyzing...\n",
            "\n",
            "- Your file contains 24904 prompt-completion pairs\n",
            "- Based on your data it seems like you're trying to fine-tune a model for classification\n",
            "- For classification, we recommend you try one of the faster and cheaper models, such as `ada`\n",
            "- For classification, you can estimate the expected model performance by keeping a held out dataset, which is not used for training\n",
            "- There are 3 examples that are very long. These are rows: [19298, 19327, 19521]\n",
            "For conditional generation, and for classification the examples shouldn't be longer than 2048 tokens.\n",
            "- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
            "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
            "\n",
            "Based on the analysis we will perform the following actions:\n",
            "- [Recommended] Remove 3 long examples [Y/n]: y\n",
            "- [Recommended] Add a suffix separator ` ->` to all prompts [Y/n]: y\n",
            "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: y\n",
            "- [Recommended] Would you like to split into training and validation set? [Y/n]: y\n",
            "\n",
            "\n",
            "Your data will be written to a new JSONL file. Proceed [Y/n]: y\n",
            "\n",
            "Wrote modified files to `./test_prepared_train.jsonl` and `./test_prepared_valid.jsonl`\n",
            "Feel free to take a look!\n",
            "\n",
            "Now use that file when fine-tuning:\n",
            "> openai api fine_tunes.create -t \"./test_prepared_train.jsonl\" -v \"./test_prepared_valid.jsonl\" --compute_classification_metrics --classification_positive_class \" 1\"\n",
            "\n",
            "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` ->` for the model to start generating completions, rather than continuing with the prompt.\n",
            "Once your model starts training, it'll approximately take 10.0 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "RDcd9ycss2h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api files.create -f ./train.jsonl -p fine-tune\n",
        "!openai api files.create -f ./test.jsonl -p fine-tune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_begK6zXmP8",
        "outputId": "1bd65086-fea5-4283-df77-fe08b6a2119b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload progress: 100% 34.0M/34.0M [00:00<00:00, 91.7Mit/s]\n",
            "{\n",
            "  \"id\": \"file-zfmfiMl9qmSuRASGR2bicdcI\",\n",
            "  \"bytes\": 33957395,\n",
            "  \"created_at\": 1719376103,\n",
            "  \"filename\": \"./train.jsonl\",\n",
            "  \"object\": \"file\",\n",
            "  \"purpose\": \"fine-tune\",\n",
            "  \"status\": \"processed\",\n",
            "  \"status_details\": null\n",
            "}\n",
            "Upload progress: 100% 34.0M/34.0M [00:00<00:00, 105Mit/s]\n",
            "{\n",
            "  \"id\": \"file-p3t98LbCjGNZx4cEEu1SpAmv\",\n",
            "  \"bytes\": 33957395,\n",
            "  \"created_at\": 1719376106,\n",
            "  \"filename\": \"./test.jsonl\",\n",
            "  \"object\": \"file\",\n",
            "  \"purpose\": \"fine-tune\",\n",
            "  \"status\": \"processed\",\n",
            "  \"status_details\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = client.files.create(file=open(\"train.jsonl\", \"rb\"), purpose='fine-tune')\n",
        "train_file_id = train_file.id\n",
        "test_file = client.files.create(file=open(\"test.jsonl\", \"rb\"), purpose='fine-tune')\n",
        "test_file_id = test_file.id"
      ],
      "metadata": {
        "id": "DjHmDK9Mxfcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !openai api completions.create \\\n",
        "# -t ./train.jsonl \\\n",
        "# --compute_classification_metrics \\\n",
        "# --classification_n_classes 2 \\\n",
        "# -m ada \\\n",
        "# --n_epochs 1\n",
        "\n",
        "fine_tune_response = client.fine_tuning.jobs.create(\n",
        "    training_file=train_file_id,\n",
        "    validation_file=test_file_id,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    hyperparameters={\n",
        "      \"n_epochs\":1,\n",
        "      # compute_classification_metrics:True,\n",
        "      # classification_n_classes:2\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "r5AQPPTUWYV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fine_tune_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeWzx6JeWK33",
        "outputId": "f6eef424-324b-486c-ad67-9e29cc29906a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FineTuningJob(id='ftjob-FLQroqtZQClWwdwdvjMnwfi0', created_at=1719376206, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=1, batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-IGE1n4VtYEKHaRRX86nVPpRS', result_files=[], seed=788634160, status='validating_files', trained_tokens=None, training_file='file-VYY9JT3jJ39IJI0v2WjxerCm', validation_file='file-0mHZPQw7Fd1rQYSyO3OfnSaI', estimated_finish=None, integrations=[], user_provided_suffix=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get token probability"
      ],
      "metadata": {
        "id": "5z5YoHgl0oJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "orQgTsfQv-jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = training_df['prompt'].sample(1).iloc[0]"
      ],
      "metadata": {
        "id": "rk98OEvj0sn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = client.chat.completions.create(\n",
        "    model='gpt-3.5-turbo-0125',\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    max_tokens=1,\n",
        "    temperature=0,\n",
        "    logprobs=True\n",
        ")"
      ],
      "metadata": {
        "id": "P_Go6anE0ysk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# probs = []\n",
        "# logprobs = res ['choices'][0]['logprobs']['top_logprobs']\n",
        "\n",
        "# for logprob in logprobs:\n",
        "#   _probs={}\n",
        "#   for key, value in logprob.items():\n",
        "#     _probs[key] = math.exp(value)\n",
        "#   probs.append(_probs)\n",
        "\n",
        "# pred = res['choices'][0].text.strip()\n",
        "\n",
        "# print(\"Prompt: \\n\", prompt[:-1])\n",
        "# print(\"Predicted Category:\", pred)\n",
        "# print(\"Probabilities:\")\n",
        "# for prob in probs:\n",
        "#     for key, value in sorted(prob.items(), key=lambda x: x[1], reverse=True):\n",
        "#         print(f\"{key}: {value:.4f}\")\n",
        "#     print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "-B-g0VN109Xk",
        "outputId": "7e76502d-4724-4ded-e40a-14c358eb434a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'ChatCompletion' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-164-c6e684257400>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'choices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logprobs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'top_logprobs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlogprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogprobs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0m_probs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'ChatCompletion' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'client' is already defined and you have a successful API response\n",
        "# Replace with your actual API call if needed\n",
        "# res = client.chat.completions.create(...)\n",
        "\n",
        "# Check if the 'choices' key exists in the response\n",
        "if 'choices' in res and res['choices']:\n",
        "    # Access the first choice\n",
        "    choice = res['choices'][0]\n",
        "\n",
        "    # Check if 'logprobs' and 'top_logprobs' keys exist within the choice\n",
        "    if 'logprobs' in choice and 'top_logprobs' in choice['logprobs']:\n",
        "        logprobs = choice['logprobs']['top_logprobs']\n",
        "        probs = []\n",
        "\n",
        "        for logprob in logprobs:\n",
        "            _probs = {}\n",
        "            for key, value in logprob.items():\n",
        "                _probs[key] = math.exp(value)\n",
        "            probs.append(_probs)\n",
        "\n",
        "        # Assuming 'text' key exists in the choice\n",
        "        pred = choice.get('text', '').strip()\n",
        "\n",
        "        print(\"Prompt: \\n\", prompt[:-1])\n",
        "        print(\"Predicted Category:\", pred)\n",
        "        print(\"Probabilities:\")\n",
        "        for prob in probs:\n",
        "            for key, value in sorted(prob.items(), key=lambda x: x[1], reverse=True):\n",
        "                print(f\"{key}: {value:.4f}\")\n",
        "            print()\n",
        "    else:\n",
        "        print(\"Error: 'logprobs' or 'top_logprobs' not found in the API response.\")\n",
        "else:\n",
        "    print(\"Error: 'choices' not found in the API response.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxUkVw1c7eRw",
        "outputId": "3f218548-c89b-4cea-aa70-4334671b3acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 'choices' not found in the API response.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-U6Qwn2x16ID"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}