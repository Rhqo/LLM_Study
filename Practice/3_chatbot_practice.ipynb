{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOyaSC2qZwtwlL9fBcWwaa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rhqo/LLM_Study/blob/main/3_chatbot_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -q\n",
        "!pip install cohere -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w1xQemVW3df",
        "outputId": "680a558a-577b-46f2-c980-8b7991f15317"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.5/327.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.8/173.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import cohere\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "ue3QWNp3W3Ml"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "co = cohere.Client(userdata.get('CO_API_KEY'))\n",
        "openai_client = OpenAI(\n",
        "    api_key=userdata.get('OPENAI_API_KEY')\n",
        ")"
      ],
      "metadata": {
        "id": "wFlKQS28ZrZL"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatGPT Q/A chatbot"
      ],
      "metadata": {
        "id": "9h5o8fAyVKj9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "s_TyFeg0U3Zh"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = '''You are a helpful Q/A bot that can only reference material from a knowledge base.\n",
        "All context was pulled from a knowledge base.\n",
        "If a user asks anything that is not \"from the knowledge base,\" say that you cannot answer.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_prompt_openai(prompt, suppress=False, model='gpt-3.5-turbo', **kwargs):\n",
        "    \" a simple function to take in a prompt and run it through a given non-chat model \"\n",
        "\n",
        "    chat_completion = openai_client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        **kwargs\n",
        "    )\n",
        "    answer = chat_completion.choices[0].message.content\n",
        "    if not suppress:\n",
        "        print(f'PROMPT:\\n------\\n{prompt}\\n------\\nRESPONSE\\n------\\n{answer}')\n",
        "    else:\n",
        "        return answer"
      ],
      "metadata": {
        "id": "a5LG8TMyfd5M"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_prompt_cohere(prompt, suppress=False, model='command-xlarge', **kwargs):\n",
        "    response = co.generate(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        **kwargs,\n",
        "      )\n",
        "    if not suppress:\n",
        "        print(f'PROMPT:\\n------\\n{prompt}\\n------\\nRESPONSE\\n------\\n{response.generations[0].text}')"
      ],
      "metadata": {
        "id": "1GqI8GU0l17W"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"===OpenAI===\")\n",
        "test_prompt_openai('Translate to Korean.\\n\\nWhere is the nearest restaurant?')\n",
        "print(\"\\n\\n===Cohere===\")\n",
        "test_prompt_cohere('Translate to Korean.\\n\\nWhere is the nearest restaurant?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbtMwwrbl5Ih",
        "outputId": "01b0f414-1ef7-4f5e-e5e9-f2716fc909d7"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===OpenAI===\n",
            "PROMPT:\n",
            "------\n",
            "Translate to Korean.\n",
            "\n",
            "Where is the nearest restaurant?\n",
            "------\n",
            "RESPONSE\n",
            "------\n",
            "가장 가까운 음식점은 어디에 있나요?\n",
            "\n",
            "\n",
            "===Cohere===\n",
            "PROMPT:\n",
            "------\n",
            "Translate to Korean.\n",
            "\n",
            "Where is the nearest restaurant?\n",
            "------\n",
            "RESPONSE\n",
            "------\n",
            " In which language would you like this query translated? I can translate this query into Korean for you:\n",
            "\n",
            "매우 가까워서 저희는 가까지 갈 수 있는 주문가가 있습니까? \n",
            "\n",
            "This translates to: \"Is there a nearby restaurant where we could order?\" \n",
            "\n",
            "Let me know if this was the language you wanted and if you have any more queries about Korean translation! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"===OpenAI===\")\n",
        "test_prompt_openai('Translate to Korean.\\n\\nenglish:Where is the nearest restaurant?\\nkorean:')\n",
        "print(\"\\n\\n===Cohere===\")\n",
        "test_prompt_cohere('Translate to Korean.\\n\\nenglish:Where is the nearest restaurant?\\nkorean:')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDfidKT-ppaA",
        "outputId": "ef09b872-2c46-4a00-9112-fecf3c3f31b2"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===OpenAI===\n",
            "PROMPT:\n",
            "------\n",
            "Translate to Korean.\n",
            "\n",
            "english:Where is the nearest restaurant?\n",
            "korean:\n",
            "------\n",
            "RESPONSE\n",
            "------\n",
            "가장 가까운 식당은 어디에 있나요?\n",
            "\n",
            "\n",
            "===Cohere===\n",
            "PROMPT:\n",
            "------\n",
            "Translate to Korean.\n",
            "\n",
            "english:Where is the nearest restaurant?\n",
            "korean:\n",
            "------\n",
            "RESPONSE\n",
            "------\n",
            " 저희는 가까이에 식당이 있는지 알 수 있을까요? \n",
            "(JuhEE nee go-ga-ee-ee-ssuh shikdang-ee isht-lee-da-ra su iss-UH-suh-guh?) \n",
            "\n",
            "Is there a restaurant nearby? \n",
            "\n",
            "Let me know if you would like any assistance translating from Korean to English as well! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatbotGPT():\n",
        "  def __init__(self, system_prompt, threshold=.8):\n",
        "    self.conversation = [{'role': 'system', 'content': system_prompt}]\n",
        "    self.threshold = threshold\n",
        "\n",
        "  def display_conversation(self):\n",
        "      '''display the conversation in a pretty format denoting the system, user and assistant differently'''\n",
        "      for turn in self.conversation:\n",
        "        role = turn['role']\n",
        "        content = turn['content']\n",
        "        if role == 'system':\n",
        "          print(f'System: {content}')\n",
        "        elif role == 'user':\n",
        "          print(f'User: {content}')\n",
        "        elif role == 'assistant':\n",
        "          print(f'Assistant: {content}')\n",
        "        print('------------')\n",
        "\n",
        "  def user_turn(self, message):\n",
        "    self.conversation.append({\"role\": \"user\", \"content\": message})\n",
        "    # best_result = get_best_result_from_pinecone(message)\n",
        "    # print(best_result)\n",
        "    # if best_result['score'] >= self.threshold:\n",
        "    #   # Add to the context to the system prompt\n",
        "    #   self.conversation[0]['content']+=f'\\n\\nFrom the knowledge base: \"{best_result[\"text\"]}\"'\n",
        "\n",
        "    chatgpt_response = openai_client.chat.completions.create(\n",
        "      model='gpt-4',\n",
        "      temperature=0,\n",
        "      messages=self.conversation\n",
        "    ).choices[0].message.content.strip()\n",
        "    self.conversation.append({'role': 'assistant', 'content': chatgpt_response})\n",
        "    return self.conversation[-1]"
      ],
      "metadata": {
        "id": "0_6Tzoc4Vl5p"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = ChatbotGPT(system_prompt=SYSTEM_PROMPT)\n",
        "\n",
        "c.user_turn('what are fixed costs?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU8D_EWNWtiR",
        "outputId": "3411eaea-0900-4d81-f08f-c6877fef5b0a"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'role': 'assistant',\n",
              " 'content': 'Fixed costs are business expenses that are not dependent on the level of goods or services produced by the business. They are one of several types of costs a business may incur, and are often time-related, such as salaries or rents being paid per month, and are often contractually agreed upon. Fixed costs are an important aspect of financial analysis for businesses, as they are costs that a business must pay, regardless of its level of output.'}"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CJpmpHd9Wt-D"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_9PBwb_Uqash"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T7bPrAJGqhbg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}